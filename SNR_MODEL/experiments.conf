
base  {
  ffnn_size = 150
  ffnn_depth = 2
  contextualization_size = 200
  contextualization_layers = 3
  lm_size = 1024
  lm_layers = 4
  lm_path = "extract_embeddings/bert_features.hdf5"

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = "extract_embeddings/train.jsonl"
  eval_path = "extract_embeddings/train.jsonl"
  lm_path = ""
  test_path = ""
  ner_types = []
  eval_frequency = 100 #500
  report_frequency = 20
  log_root = logs
  max_step = 100
}


# Main configuration.
pt_law = ${base}{
  train_path = extract_embeddings/train.jsonl
  test_path = extract_embeddings/train.jsonl
  lm_path = extract_embeddings/bert_features.hdf5
  ner_types = ["DEF","OBLIG","RIGHT","LEFFECT","INTRO","IGNORE","REF","LREF","TREF","URL","NE_","NE_ADM","NE_ORG","NE_OFFICE","NE_PERSON","TIME_","TIME_DATE_ABS","TIME_DATE_REL_ENUNC","TIME_DATE_REL_TEXT","TIME_DURATION","TIME_FREQ","DEFINIENDUM","DEFINIENS","DEF-INCLUSION","DEF-EXCLUSION","DEF-EXEMPLUM","SCOPE","ACTION","CONDITION","CONCESSION","PURPOSE","EXPERIENCER","AGENT","PATIENT","OBJECT","THEME","EXCEPTION","EFFECT","RATIONALE","SITUATION","RESULT","NEG"]
  max_step = 100
  flat_ner = false
}

